# # Infant Monitory System
# 
# The Infant Monitoring System project powered by Machine Learning is a groundbreaking endeavor that combines cutting-edge technology with the utmost concern for infant safety and well-being. This project aims to create a sophisticated and intelligent system that assists caregivers in monitoring and ensuring the health and safety of infants in real-time.
# 
# Using state-of-the-art machine learning algorithms, this system is designed to process and analyze various data inputs strategically placed around the infant's environment. The machine learning models integrated into the system can learn and understand normal patterns of infant behavior and environmental conditions.

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.stats import ttest_ind
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import IsolationForest
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from Function_Calling import split_and_convert_bp_column,data_clean

import warnings
warnings.filterwarnings('ignore')


# ### Data Preprocessing
# #### Data Collection
# Data was generated by https://www.mockaroo.com/
# 
# This dataset consists of 1,000 rows and includes various columns representing different attributes related to hypothetical infant health monitoring. Each row in the dataset corresponds to an individual infant's health profile.

# In[2]:


# Load the dataset
filename = "train.csv"
dataset = pd.read_csv(filename)
dataset.head()


# In[3]:


dataset.dtypes


# #### Data Preprocessing



split_and_convert_bp_column(dataset)


# #### Data Cleaning

# In[6]:


# 1. Handling Missing Values
dataset.dropna()  # Remove rows with any missing values
dataset.fillna(0)  # Replace missing values with a specified value
dataset.interpolate()  # Interpolate missing values based on neighboring values

# 2. Handling Duplicates
dataset.drop_duplicates()  # Remove duplicate rows

# 3. Addressing Irrelevant Data
irrelevant_columns = ['Family_Income', 'Parental_Education', 'Sleep_Duration_Hrs', 'ID']
dataset = dataset.drop(columns=irrelevant_columns)

# 4. Boolean Encoding
columns_to_convert = ['Fever', 'Cough', 'Runny_Nose', 'Skin_Rash', 'Vomiting', 'Diarrhea']
dataset[columns_to_convert] = dataset[columns_to_convert].astype(int)
dataset['Feeding_Method'] = dataset['Feeding_Method'].replace({'Formula': 2, 'Breastfed': 1, 'Mixed': 0})
dataset['Immunization_Status'] = dataset['Immunization_Status'].replace({'Complete': 1, 'Incomplete': 0})
dataset['Infant_Status'] = dataset['Infant_Status'].replace({'Healthy': 0, 'Sick': 1})

dataset.head()


# ### Exploratory Data Analysis (EDA)

# In[7]:


# Summary Statistics
summary_stats = dataset.describe()
print(summary_stats)


# In[8]:


# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(dataset['Age_Months'], bins=20, kde=True)
plt.title('Distribution of Age in Months')
plt.xlabel('Age (Months)')
plt.ylabel('Frequency')
plt.show()


# In[9]:


# plotting with target feature
sns.countplot(data=dataset, x='Infant_Status', palette='Set2')
plt.title('Count of Infant Health Status')
plt.xlabel('Infant Status')
plt.ylabel('Count')
plt.show()


# In[10]:


health_attributes = ['Oxygen_Saturation', 'Pulse_Rate', 'Temperature_C']

# Pair Plot for Health Attributes
sns.set_palette("Paired")  # Set color palette
sns.pairplot(dataset, vars=health_attributes, hue='Infant_Status', diag_kind='hist')
plt.suptitle('Pair Plot of Health Attributes with Different Colors')
plt.show()

# Correlation Matrix (Heatmap) for Health Attributes
correlation_matrix = dataset[health_attributes].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Health Attributes')
plt.show()

# Box Plots by Infant Status
plt.figure(figsize=(12, 8))
for attribute in health_attributes:
    plt.subplot(2, 2, health_attributes.index(attribute) + 1)
    sns.boxplot(x='Infant_Status', y=attribute, data=dataset)
    plt.title(f'Box Plot of {attribute} by Infant Status')
    plt.xlabel('Infant Status')
    plt.ylabel(attribute)
plt.tight_layout()
plt.show()

# Histograms by Infant Status
plt.figure(figsize=(12, 8))
for attribute in health_attributes:
    plt.subplot(2, 2, health_attributes.index(attribute) + 1)
    sns.histplot(data=dataset, x=attribute, hue='Infant_Status', multiple='stack')
    plt.title(f'Histogram of {attribute} by Infant Status')
    plt.xlabel(attribute)
    plt.ylabel('Count')
plt.tight_layout()
plt.show()


# In[11]:


# Feature Importance using Random Forest
X = dataset[health_attributes]
y = dataset['Infant_Status']

model = DecisionTreeClassifier(random_state=42)
model.fit(X, y)

feature_importances = model.feature_importances_
plt.figure(figsize=(8, 6))
sns.barplot(x=feature_importances, y=health_attributes)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Health Attributes')
plt.show()

# Statistical Tests (e.g., t-tests)
for attribute in health_attributes:
    sick_values = dataset[dataset['Infant_Status'] == 1][attribute]
    healthy_values = dataset[dataset['Infant_Status'] == 0][attribute]
    t_stat, p_value = ttest_ind(sick_values, healthy_values)
    print(f"{attribute}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}")

# Dimensionality Reduction (PCA)
pca = PCA(n_components=2)  # We'll visualize using two principal components
X_pca = pca.fit_transform(dataset.drop(columns=['Infant_Status']))

# Create a scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=dataset['Infant_Status'], palette='Set2')
plt.title('PCA Plot of Infant Health Attributes')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
# plt.legend(title='Infant Status', loc='upper right', labels=['Healthy', 'Sick'])
plt.show()

# Clustering (K-means)
kmeans = KMeans(n_clusters=2, random_state=42)
cluster_labels = kmeans.fit_predict(X)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cluster_labels, palette='Set2')
plt.title('K-means Clustering of Health Attributes')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()


# In[12]:


# Calculate the correlation matrix
correlation_matrix = dataset.corr(numeric_only=True)

# Get the correlation of "Infant_Status" with other variables and drop the "Infant_Status" row
infant_status_correlation = correlation_matrix['Infant_Status'].drop('Infant_Status')

# Plot the correlation heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True)

# Plot the bar plot of correlation values
sns.barplot(x=infant_status_correlation.index, y=infant_status_correlation.values)
plt.title('Correlation of Infant Health Status with Other Variables')
plt.xlabel('Variables')
plt.ylabel('Correlation')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


# ### Data Splitting

# In[13]:


# Split the dataset into feature data (X) and label data (y)
X = dataset.drop(columns='Infant_Status')
y = dataset['Infant_Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)


# ### Model Selection

# In[14]:


# Train and Predict using Different Models
# Create a Random Forest Classifier model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
rf_pred = model.predict(X_test)

# Logistic Regression
logreg_model = LogisticRegression(random_state=42)
logreg_model.fit(X_train, y_train)
logreg_pred = logreg_model.predict(X_test)

# Support Vector Machine (SVM)
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
svm_pred = svm_model.predict(X_test)

# Gradient Boosting
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)

# # K-Nearest Neighbors (KNN)
# knn_model = KNeighborsClassifier()
# knn_model.fit(X_train, y_train)
# knn_pred = knn_model.predict(X_test)

# Naive Bayes
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_pred = nb_model.predict(X_test)

# Neural Network (Multi-layer Perceptron)
nn_model = MLPClassifier(random_state=42)
nn_model.fit(X_train, y_train)
nn_pred = nn_model.predict(X_test)

# Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)

# AdaBoost
adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)
adaboost_model.fit(X_train, y_train)
adaboost_pred = adaboost_model.predict(X_test)

# Print predictions for each model
print("Logistic Regression Predictions:", logreg_pred)
print("SVM Predictions:", svm_pred)
print("Gradient Boosting Predictions:", gb_pred)
# print("KNN Predictions:", knn_pred)
print("Naive Bayes Predictions:", nb_pred)
print("Neural Network Predictions:", nn_pred)
print("Decision Tree Predictions:", dt_pred)
print("AdaBoost Predictions:", adaboost_pred)
print("Random Forest Predictions:", rf_pred)


# In[15]:


# Calculate Accuracy for Each Model
logreg_accuracy = accuracy_score(y_test, logreg_pred)
svm_accuracy = accuracy_score(y_test, svm_pred)
gb_accuracy = accuracy_score(y_test, gb_pred)
# knn_accuracy = accuracy_score(y_test, knn_pred)
nb_accuracy = accuracy_score(y_test, nb_pred)
nn_accuracy = accuracy_score(y_test, nn_pred)
dt_accuracy = accuracy_score(y_test, dt_pred)
adaboost_accuracy = accuracy_score(y_test, adaboost_pred)
rf_accuracy = accuracy_score(y_test, rf_pred)

# Print Accuracy, Confusion matrix and Classification report for Each Model
print(f"Logistic Regression Accuracy: {logreg_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(logreg_pred, y_test)}')
print(f'Classification report :\n {classification_report(logreg_pred, y_test)}')

print(f"SVM Accuracy: {svm_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(svm_pred, y_test)}')
print(f'Classification report :\n {classification_report(svm_pred, y_test)}')

print(f"Gradient Boosting Accuracy: {gb_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(gb_pred, y_test)}')
print(f'Classification report :\n {classification_report(gb_pred, y_test)}')

# print(f"KNN Accuracy: {knn_accuracy * 100:.2f}%")
# print(f'Confusion matrix :\n {confusion_matrix(knn_pred, y_test)}')
# print(f'Classification report :\n {classification_report(knn_pred, y_test)}')

print(f"Naive Bayes Accuracy: {nb_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(nb_pred, y_test)}')
print(f'Classification report :\n {classification_report(nb_pred, y_test)}')

print(f"Neural Network Accuracy: {nn_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(nn_pred, y_test)}')
print(f'Classification report :\n {classification_report(nn_pred, y_test)}')

print(f"Decision Tree Accuracy: {dt_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(dt_pred, y_test)}')
print(f'Classification report :\n {classification_report(dt_pred, y_test)}')

print(f"AdaBoost Accuracy: {adaboost_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(adaboost_pred, y_test)}')
print(f'Classification report :\n {classification_report(adaboost_pred, y_test)}')

print(f"Random Forest Accuracy: {rf_accuracy * 100:.2f}%")
print(f'Confusion matrix :\n {confusion_matrix(rf_pred, y_test)}')
print(f'Classification report :\n {classification_report(rf_pred, y_test)}')


# In[16]:


# Calculate ROC curve and AUC
y_prob = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()


# In[17]:


# Define hyperparameter grids for different models
param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'max_iter': [100, 200, 300]
}

param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}

# param_grid_knn = {
#     'n_neighbors': [3, 5, 7],
#     'p': [1, 2]
# }

param_grid_nn = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
    'alpha': [0.0001, 0.001, 0.01]
}

param_grid_dt = {
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

param_grid_adaboost = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Create GridSearchCV instances for different models
grid_search_lr = GridSearchCV(LogisticRegression(random_state=42), param_grid_lr, cv=5)
grid_search_svm = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=5)
grid_search_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gb, cv=5)
# grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_search_nn = GridSearchCV(MLPClassifier(random_state=42), param_grid_nn, cv=5)
grid_search_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=5)
grid_search_adaboost = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid_adaboost, cv=5)
grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5)

# List of models and their corresponding grid search instances
models = [
    ("Logistic Regression", grid_search_lr),
    ("SVM", grid_search_svm),
    ("Gradient Boosting", grid_search_gb),
    # ("KNN", grid_search_knn),
    ("Neural Network", grid_search_nn),
    ("Decision Tree", grid_search_dt),
    ("AdaBoost", grid_search_adaboost),
    ("Random Forest", grid_search_rf)
]

for model_name, grid_search in models:
    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"{model_name} Accuracy: {accuracy:.2%}")
    print(f"Best Parameters: {best_params}")
    print("=" * 40)


# In[18]:


# Initialize a list to store results for each model
results = []

# Iterate over each model and its corresponding grid search instance
for model_name, grid_search in models:
    if model_name == "Naive Bayes":
        best_model = grid_search
        best_model.fit(X_train, y_train)  # Fit Naive Bayes model
        y_pred = best_model.predict(X_test)
    else:
        grid_search.fit(X_train, y_train)
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test)
    
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    # Confusion matrix
    confusion = confusion_matrix(y_test, y_pred)
    
    # Classification report
    classification_rep = classification_report(y_test, y_pred)
    
    # Store results in the list
    results.append({
        "Model": model_name,
        "Best Model": best_model,
        "Accuracy": accuracy,
        "Confusion Matrix": confusion,
        "Classification Report": classification_rep
    })

# Print the results for each model
for result in results:
    print(f"Model: {result['Model']}")
    print(f"Accuracy: {result['Accuracy']:.2%}")
    print("Confusion Matrix:")
    print(result['Confusion Matrix'])
    print("Classification Report:")
    print(result['Classification Report'])
    print("=" * 40)


# #### Model Saving

# In[19]:


# Save the model to a file
filename = 'Model.joblib'
joblib.dump(grid_search_gb, filename)

joblib.dump(dt_model, 'dt_model.joblib')


# #### Model Calling 

# In[22]:


# # Load the model from the file
# loaded_model = joblib.load('Model.joblib')

# # Load Test Data set
# filename2 = "convert.csv"
# datasetTrain = pd.read_csv(filename2)

# split_and_convert_bp_column(datasetTrain)

# X_testA = data_clean(datasetTrain)

# predA = dt_model.predict(X_testA)

# # Print the results
# print("Loaded Model Prediction:", predA)


# ### Results
# We trained and evaluated several machine learning models to predict infant health based on various attributes, including vital signs and other health-related features. The models included Logistic Regression, Support Vector Machine (SVM), Gradient Boosting, K-Nearest Neighbors (KNN), Naive Bayes, Neural Network, Decision Tree, and Random Forest.
# 
# #### Discussion of Model Performance
# 
# ##### Decision Tree Outperforms Other Models
# 
# Among the tested models, the Decision Tree model exhibited the highest accuracy of 70.00%. This result suggests that the Decision Tree algorithm is well-suited for predicting infant health based on the provided attributes. The high accuracy may be attributed to the ability of Decision Trees to capture complex relationships within the data.
# 
# ##### Neural Network and KNN Show Promising Results
# 
# The Neural Network and K-Nearest Neighbors (KNN) models also demonstrated promising performance, achieving accuracy rates of 40.00%. These models are capable of learning intricate patterns in the data, which could contribute to their effectiveness in predicting infant health.
# 
# ##### Limited Performance of Baseline Models
# 
# We also compared our models against baseline models that rely on simplistic assumptions. The baseline models achieved significantly lower accuracy rates, highlighting the advantage of employing more advanced machine learning techniques for accurate predictions.
# 
# ### Conclusion
# 
# In this section, we presented the results of our model development process and discussed their performance in predicting infant health. The Decision Tree model emerged as the most accurate, while the Neural Network and KNN models also demonstrated promising results. Our models significantly outperformed baseline models, reaffirming the value of employing advanced machine learning techniques in the Infant Health Monitoring System.
# 
# Further research could explore the incorporation of additional features and more complex model architectures to improve prediction accuracy and contribute to the enhancement of infant healthcare.
